{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67a00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574bf8f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset is now located at: data/UCI HAR Dataset/\n",
      "X_train: (7352, 128, 9)\n",
      "X_test: (2947, 128, 9)\n",
      "y_train: (7352, 6)\n",
      "y_test: (2947, 6)\n",
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "features shape, labels shape, each features mean, each features standard deviation\n",
      "(2947, 128, 9) (2947, 6) 0.09913992 0.39567086\n",
      "the dataset is therefore properly normalised, as expected.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n    relative to D:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python:\n\n    ops\\rnn_cell_impl.py:1312 call\n        cur_inp, new_state = cell(cur_inp, cur_state)\n    ops\\rnn_cell_impl.py:386 __call__\n        self, inputs, state, scope=scope, *args, **kwargs)\n    layers\\base.py:548 __call__\n        outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    keras\\engine\\base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    keras\\engine\\base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    keras\\utils\\tf_utils.py:306 wrapper\n        output_shape = fn(instance, input_shape)\n    ops\\rnn_cell_impl.py:735 build\n        shape=[input_depth + h_depth, 4 * self._num_units])\n    util\\deprecation.py:324 new_func\n        return func(*args, **kwargs)\n    keras\\engine\\base_layer.py:1702 add_variable\n        return self.add_weight(*args, **kwargs)\n    layers\\base.py:461 add_weight\n        **kwargs)\n    keras\\engine\\base_layer.py:522 add_weight\n        aggregation=aggregation)\n    training\\tracking\\base.py:744 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    ops\\variable_scope.py:1504 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:1247 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:550 get_variable\n        return custom_getter(**custom_getter_kwargs)\n    ops\\rnn_cell_impl.py:247 _rnn_get_variable\n        variable = getter(*args, **kwargs)\n    ops\\variable_scope.py:519 _true_getter\n        aggregation=aggregation)\n    ops\\variable_scope.py:860 _get_single_variable\n        raise ValueError(err_msg)\n\n    ValueError: Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_1984/1140411296.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mpred_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_Network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;31m# Loss,optimizer,evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_1984/1140411296.py\u001b[0m in \u001b[0;36mLSTM_Network\u001b[1;34m(_X, config)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mlstm_cells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlstm_cell_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_cell_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;31m# Get LSTM cell output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_cells\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;31m#print (np.array(outputs).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Get last time step's output feature for a \"many to one\" style classifier,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[1;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[0;32m   1436\u001b[0m             state_size=cell.state_size)\n\u001b[0;32m   1437\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \u001b[1;31m# Keras RNN cells only return state as list, even if it's a single tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1423\u001b[0m         \u001b[0mvarscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m       \u001b[1;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1425\u001b[1;33m       \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1426\u001b[0m       \u001b[1;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n    relative to D:\\Anaconda Python\\envs\\tensorflow2-cpu\\lib\\site-packages\\tensorflow_core\\python:\n\n    ops\\rnn_cell_impl.py:1312 call\n        cur_inp, new_state = cell(cur_inp, cur_state)\n    ops\\rnn_cell_impl.py:386 __call__\n        self, inputs, state, scope=scope, *args, **kwargs)\n    layers\\base.py:548 __call__\n        outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    keras\\engine\\base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    keras\\engine\\base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    keras\\utils\\tf_utils.py:306 wrapper\n        output_shape = fn(instance, input_shape)\n    ops\\rnn_cell_impl.py:735 build\n        shape=[input_depth + h_depth, 4 * self._num_units])\n    util\\deprecation.py:324 new_func\n        return func(*args, **kwargs)\n    keras\\engine\\base_layer.py:1702 add_variable\n        return self.add_weight(*args, **kwargs)\n    layers\\base.py:461 add_weight\n        **kwargs)\n    keras\\engine\\base_layer.py:522 add_weight\n        aggregation=aggregation)\n    training\\tracking\\base.py:744 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    ops\\variable_scope.py:1504 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:1247 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:550 get_variable\n        return custom_getter(**custom_getter_kwargs)\n    ops\\rnn_cell_impl.py:247 _rnn_get_variable\n        variable = getter(*args, **kwargs)\n    ops\\variable_scope.py:519 _true_getter\n        aggregation=aggregation)\n    ops\\variable_scope.py:860 _get_single_variable\n        raise ValueError(err_msg)\n\n    ValueError: Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?\n"
     ]
    }
   ],
   "source": [
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "\n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_ - 1\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"\n",
    "    define a class to store parameters,\n",
    "    the input should be feature mat of training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, X_test):\n",
    "        # Input data\n",
    "        self.train_count = len(X_train)  # 7352 training series\n",
    "        self.test_data_count = len(X_test)  # 2947 testing series\n",
    "        self.n_steps = len(X_train[0])  # 128 time_steps per series\n",
    "\n",
    "        # Training\n",
    "        self.learning_rate = 0.0025\n",
    "        self.lambda_loss_amount = 0.0015\n",
    "        self.training_epochs = 300\n",
    "        self.batch_size = 1500\n",
    "\n",
    "        # LSTM structure\n",
    "        self.n_inputs = len(X_train[0][0])  # Features count is of 9: 3 * 3D sensors features over time\n",
    "        self.n_hidden = 32  # nb of neurons inside the neural network\n",
    "        self.n_classes = 6  # Final output classes\n",
    "        self.W = {\n",
    "            'hidden': tf.Variable(tf.random.normal([self.n_inputs, self.n_hidden])),\n",
    "            'output': tf.Variable(tf.random.normal([self.n_hidden, self.n_classes]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'hidden': tf.Variable(tf.random.normal([self.n_hidden], mean=1.0)),\n",
    "            'output': tf.Variable(tf.random.normal([self.n_classes]))\n",
    "        }\n",
    "\n",
    "\n",
    "def LSTM_Network(_X, config):\n",
    "    \"\"\"Function returns a TensorFlow RNN with two stacked LSTM cells\n",
    "\n",
    "    Two LSTM cells are stacked which adds deepness to the neural network.\n",
    "    Note, some code of this notebook is inspired from an slightly different\n",
    "    RNN architecture used on another dataset, some of the credits goes to\n",
    "    \"aymericdamien\".\n",
    "\n",
    "    Args:\n",
    "        _X:     ndarray feature matrix, shape: [batch_size, time_steps, n_inputs]\n",
    "        config: Config for the neural network.\n",
    "\n",
    "    Returns:\n",
    "        This is a description of what is returned.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Raises an exception.\n",
    "\n",
    "      Args:\n",
    "        feature_mat: ndarray fature matrix, shape=[batch_size,time_steps,n_inputs]\n",
    "        config: class containing config of network\n",
    "      return:\n",
    "              : matrix  output shape [batch_size,n_classes]\n",
    "    \"\"\"\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, config.n_inputs])\n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "\n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, config.W['hidden']) + config.biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, config.n_steps, 0)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.compat.v1.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    #print (np.array(outputs).shape)\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier,\n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, config.W['output']) + config.biases['output']\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    \"\"\"\n",
    "    Function to encode output labels from number indexes.\n",
    "\n",
    "    E.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \"\"\"\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 1: load and prepare data\n",
    "    # -----------------------------\n",
    "\n",
    "    # Those are separate normalised input features for the neural network\n",
    "    INPUT_SIGNAL_TYPES = [\n",
    "        \"body_acc_x_\",\n",
    "        \"body_acc_y_\",\n",
    "        \"body_acc_z_\",\n",
    "        \"body_gyro_x_\",\n",
    "        \"body_gyro_y_\",\n",
    "        \"body_gyro_z_\",\n",
    "        \"total_acc_x_\",\n",
    "        \"total_acc_y_\",\n",
    "        \"total_acc_z_\"\n",
    "    ]\n",
    "\n",
    "    # Output classes to learn how to classify\n",
    "    LABELS = [\n",
    "        \"WALKING\",\n",
    "        \"WALKING_UPSTAIRS\",\n",
    "        \"WALKING_DOWNSTAIRS\",\n",
    "        \"SITTING\",\n",
    "        \"STANDING\",\n",
    "        \"LAYING\"\n",
    "    ]\n",
    "\n",
    "    DATA_PATH = \"data/\"\n",
    "    DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "    print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n",
    "    TRAIN = \"train/\"\n",
    "    TEST = \"test/\"\n",
    "\n",
    "    X_train_signals_paths = [\n",
    "        DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "    ]\n",
    "    X_test_signals_paths = [\n",
    "        DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "    ]\n",
    "    X_train = load_X(X_train_signals_paths)\n",
    "    X_test = load_X(X_test_signals_paths)\n",
    "    print ('X_train:',X_train.shape)\n",
    "    print ('X_test:',X_test.shape)\n",
    "    y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "    y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "    y_train = one_hot(load_y(y_train_path))\n",
    "    y_test = one_hot(load_y(y_test_path))\n",
    "    print ('y_train:',y_train.shape)\n",
    "    print ('y_test:',y_test.shape)\n",
    "    # -----------------------------------\n",
    "    # Step 2: define parameters for model\n",
    "    # -----------------------------------\n",
    "\n",
    "    config = Config(X_train, X_test)\n",
    "    print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "    print(\"features shape, labels shape, each features mean, each features standard deviation\")\n",
    "    print(X_test.shape, y_test.shape,\n",
    "          np.mean(X_test), np.std(X_test))\n",
    "    print(\"the dataset is therefore properly normalised, as expected.\")\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # Step 3: Let's get serious and build the neural network\n",
    "    # ------------------------------------------------------\n",
    "    \n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    X = tf.compat.v1.placeholder(tf.float32, [None, config.n_steps, config.n_inputs])\n",
    "    Y = tf.compat.v1.placeholder(tf.float32, [None, config.n_classes])\n",
    "\n",
    "    pred_Y = LSTM_Network(X, config)\n",
    "\n",
    "    # Loss,optimizer,evaluation\n",
    "    l2 = config.lambda_loss_amount * \\\n",
    "        sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    # Softmax loss and L2\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=pred_Y)) + l2\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred_Y, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 4: Hooray, now train the neural network\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Note that log_device_placement can be turned ON but will cause console spam with RNNs.\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=False))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    # Start training for each batch and loop epochs\n",
    "    for i in range(config.training_epochs):\n",
    "        for start, end in zip(range(0, config.train_count, config.batch_size),\n",
    "                              range(config.batch_size, config.train_count + 1, config.batch_size)):\n",
    "            sess.run(optimizer, feed_dict={X: X_train[start:end],\n",
    "                                           Y: y_train[start:end]})\n",
    "\n",
    "        # Test completely at every epoch: calculate accuracy\n",
    "        pred_out, accuracy_out, loss_out = sess.run(\n",
    "            [pred_Y, accuracy, cost],\n",
    "            feed_dict={\n",
    "                X: X_test,\n",
    "                Y: y_test\n",
    "            }\n",
    "        )\n",
    "        print(\"traing iter: {},\".format(i) +\n",
    "              \" test accuracy : {},\".format(accuracy_out) +\n",
    "              \" loss : {}\".format(loss_out))\n",
    "        best_accuracy = max(best_accuracy, accuracy_out)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"final test accuracy: {}\".format(accuracy_out))\n",
    "    print(\"best epoch's test accuracy: {}\".format(best_accuracy))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a164d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow2-CPU",
   "language": "python",
   "name": "tensorflow2-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
